import os
import sys
import logging
import time
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import onnx
import onnxruntime as ort

# ===================== é…ç½®é¡¹ï¼ˆç»Ÿä¸€ç®¡ç†è·¯å¾„å’Œå‚æ•°ï¼‰ =====================
# åŸºç¡€é…ç½® - å¯æ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹
CONFIG = {
    "tokenizer_dir": "/to/path/Fun-ASR-Nano-2512/onnx",
    "onnx_model_dir": "/to/path/Fun-ASR-Nano-2512/onnx",
    "audio_test_path": "/to/path/Fun-ASR-Nano-2512/example/zh.mp3", 
    "blank_id_default": 60514,
    "target_seq_len": 0,  # å¤§äº0æ—¶é™åˆ¶è§£ç æ—¶é•¿(512â‰ˆ30ç§’)
    "warmup_runs": 3,
    "benchmark_runs": 5,
    "intra_op_num_threads": 1,  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    "inter_op_num_threads": 1,  # æ ¹æ®ä»»åŠ¡å¹¶è¡Œåº¦è°ƒæ•´
    "audio_sample_rate": 16000,  # éŸ³é¢‘é‡‡æ ·ç‡
    "device_type": "CUDA" if torch.cuda.is_available() else "CPU",
}


# ===================== Tokenizer æ³¨å†Œ =====================
def SenseVoiceTokenizer(**kwargs):
    """SenseVoiceåˆ†è¯å™¨"""
    try:
        from funasr.models.sense_voice.whisper_lib.tokenizer import get_tokenizer
    except ImportError:
        raise ImportError("è¯·å®‰è£… openai-whisperï¼špip install -U openai-whisper")

    language = kwargs.get("language", None)
    task = kwargs.get("task", None)
    is_multilingual = kwargs.get("is_multilingual", True)
    num_languages = kwargs.get("num_languages", 8749)
    vocab_path = kwargs.get("vocab_path", None)

    # æ ¡éªŒ vocab_path æ˜¯å¦å­˜åœ¨
    if vocab_path and not os.path.exists(vocab_path):
        raise FileNotFoundError(f"æŒ‡å®šçš„vocabæ–‡ä»¶ä¸å­˜åœ¨ï¼š{vocab_path}")

    tokenizer = get_tokenizer(
        multilingual=is_multilingual,
        num_languages=num_languages,
        language=language,
        task=task,
        vocab_path=vocab_path,
    )
    return tokenizer


# ===================== CTCæ¨ç†å™¨ =====================
class CTCInference:
    """CTCæ¨¡å‹æ¨ç†å™¨ï¼ˆä»…åŠ è½½å’Œæ¨ç†ONNXæ¨¡å‹ï¼‰"""

    def __init__(
            self,
            encoder_onnx_path: str,
            decoder_onnx_path: str,
            blank_id: int = CONFIG["blank_id_default"],
            target_seq_len: int = CONFIG["target_seq_len"]
    ):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            encoder_onnx_path: ç¼–ç å™¨ONNXè·¯å¾„
            decoder_onnx_path: è§£ç å™¨ONNXè·¯å¾„
            blank_id: ç©ºç™½æ ‡è®°ID
            target_seq_len: ç›®æ ‡åºåˆ—é•¿åº¦
        """
        self.encoder_onnx_path = encoder_onnx_path
        self.decoder_onnx_path = decoder_onnx_path
        self.blank_id = blank_id
        self.target_seq_len = target_seq_len

        # æ€§èƒ½ç»Ÿè®¡
        self.inference_stats = {
            "audio_load_time": 0,
            "feature_extract_time": 0,
            "encoder_infer_time": 0,
            "decoder_infer_time": 0,
            "decode_time": 0,
            "total_infer_time": 0
        }

        # ç³»ç»Ÿé…ç½®ä¿¡æ¯
        self.intra_op_num_threads = CONFIG["intra_op_num_threads"]
        self.inter_op_num_threads = CONFIG["inter_op_num_threads"]
        self.device_type = CONFIG["device_type"]
        self.audio_sample_rate = CONFIG["audio_sample_rate"]

        # åŠ è½½ONNXæ¨¡å‹
        self.encoder_session = self._load_onnx_model(encoder_onnx_path, "ç¼–ç å™¨")
        self.decoder_session = self._load_onnx_model(decoder_onnx_path, "è§£ç å™¨")

        # åˆå§‹åŒ–éŸ³é¢‘å‰ç«¯å¤„ç†
        self.frontend = self._init_frontend()

    def _load_onnx_model(self, model_path: str, model_type: str = "æ¨¡å‹"):
        """åŠ è½½ONNXæ¨¡å‹"""
        if not os.path.exists(model_path):
            logging.error(f"{model_type}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None

        try:
            # é…ç½®ONNX Runtime
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
            sess_options.enable_mem_pattern = True
            sess_options.enable_cpu_mem_arena = True
            sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            sess_options.intra_op_num_threads = self.intra_op_num_threads
            sess_options.inter_op_num_threads = self.inter_op_num_threads

            # è®¾ç½®æ‰§è¡Œæä¾›è€…
            providers = ['CUDAExecutionProvider'] if self.device_type == 'CUDA' else ['CPUExecutionProvider']

            # åŠ è½½æ¨¡å‹
            session = ort.InferenceSession(
                model_path,
                sess_options=sess_options,
                providers=providers
            )

            logging.info(f"{model_type} ONNXæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return session

        except Exception as e:
            logging.error(f"{model_type} ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def _init_frontend(self):
        """åˆå§‹åŒ–éŸ³é¢‘å‰ç«¯å¤„ç†"""
        try:
            from funasr.register import tables

            frontend_conf = {
                "fs": self.audio_sample_rate,
                "window": "hamming",
                "n_mels": 80,
                "frame_length": 25,
                "frame_shift": 10,
                "lfr_m": 7,
                "lfr_n": 6,
                "dither": 0,
                "snip_edges": True,
                "cmvn_file": None
            }

            frontend_class = tables.frontend_classes.get("wav_frontend")
            frontend = frontend_class(**frontend_conf)
            return frontend
        except Exception as e:
            logging.warning(f"åˆå§‹åŒ–éŸ³é¢‘å‰ç«¯å¤±è´¥: {e}")
            return None

    def _pad_or_truncate_encoder_output(self, encoder_out_np: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """è°ƒæ•´ç¼–ç å™¨è¾“å‡ºé•¿åº¦åˆ°ç›®æ ‡é•¿åº¦"""
        batch_size, seq_len, feat_dim = encoder_out_np.shape

        padded_encoder_out = np.zeros((batch_size, self.target_seq_len, feat_dim), dtype=np.float32)
        valid_len = min(seq_len, self.target_seq_len)
        padded_encoder_lens = np.array([valid_len] * batch_size, dtype=np.int64)
        padded_encoder_out[:, :valid_len, :] = encoder_out_np[:, :valid_len, :]

        logging.info(f"ç¼–ç å™¨è¾“å‡ºé•¿åº¦è°ƒæ•´: {seq_len} -> {valid_len} (ç›®æ ‡é•¿åº¦: {self.target_seq_len})")

        return padded_encoder_out, padded_encoder_lens

    def calculate_rtf(self, infer_time: float, audio_duration: float) -> float:
        """è®¡ç®—å®æ—¶å› å­ï¼ˆReal-Time Factorï¼‰"""
        if audio_duration <= 0:
            return float('inf')

        rtf = infer_time / audio_duration
        logging.info(f"RTFè®¡ç®—: æ¨ç†è€—æ—¶={infer_time:.3f}s, éŸ³é¢‘æ—¶é•¿={audio_duration:.3f}s, RTF={rtf:.4f}")
        return rtf

    def _load_and_process_audio(self, audio_path: str) -> Tuple[np.ndarray, np.ndarray, float]:
        """åŠ è½½å¹¶å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œæå–ç‰¹å¾"""
        try:
            from funasr.utils.load_utils import load_audio_text_image_video, extract_fbank

            # åŠ è½½éŸ³é¢‘
            load_start = time.time()
            data_src = load_audio_text_image_video(audio_path, fs=self.audio_sample_rate)
            audio_duration = len(data_src) / self.audio_sample_rate
            load_time = time.time() - load_start

            # æå–å£°å­¦ç‰¹å¾
            feat_start = time.time()
            speech, speech_lengths = extract_fbank(
                data_src,
                data_type="sound",
                frontend=self.frontend,
                is_final=True,
            )
            feat_time = time.time() - feat_start

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            speech_np = speech.cpu().numpy().astype(np.float32)
            speech_lengths_np = speech_lengths.cpu().numpy().astype(np.int64)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.inference_stats["audio_load_time"] = load_time
            self.inference_stats["feature_extract_time"] = feat_time

            logging.info(
                f"éŸ³é¢‘å¤„ç†å®Œæˆ: æ—¶é•¿={audio_duration:.2f}s, é‡‡æ ·ç‡={self.audio_sample_rate}Hz, ç‰¹å¾å½¢çŠ¶={speech_np.shape}")

            return speech_np, speech_lengths_np, audio_duration

        except Exception as e:
            logging.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            raise

    def _decode_ctc_logits(
            self,
            ctc_logits: np.ndarray,
            lengths: np.ndarray,
            tokenizer=None
    ) -> List[Dict]:
        """è§£ç CTC logitsä¸ºæ–‡æœ¬"""
        results = []
        batch_size = ctc_logits.shape[0]

        for i in range(batch_size):
            seq_len = min(lengths[i], ctc_logits.shape[1])
            logits = ctc_logits[i, :seq_len, :]

            # è´ªå¿ƒè§£ç 
            yseq = np.argmax(logits, axis=-1)

            # ç§»é™¤é‡å¤å’Œç©ºç™½æ ‡è®°
            prev_token = -1
            decoded_tokens = []
            for token in yseq:
                if token != prev_token and token != self.blank_id:
                    decoded_tokens.append(token)
                prev_token = token

            # è§£ç ä¸ºæ–‡æœ¬
            text = ""
            if tokenizer is not None:
                try:
                    text = tokenizer.decode(decoded_tokens)
                except Exception as e:
                    logging.warning(f"tokenizerè§£ç å¤±è´¥: {e}")
                    text = f"Tokens: {decoded_tokens[:20]}..."  # æˆªæ–­è¿‡é•¿çš„tokenåˆ—è¡¨
            else:
                text = f"Tokens: {decoded_tokens[:20]}..."

            results.append({
                "text": text,
                "tokens": decoded_tokens,
                "raw_tokens": yseq.tolist(),
                "sequence_length": seq_len
            })

        return results

    def inference_from_audio(
            self,
            audio_path: str,
            tokenizer=None,
            warmup_runs: int = CONFIG["warmup_runs"],
            benchmark_runs: int = CONFIG["benchmark_runs"]
    ) -> Dict:
        """
        ä»éŸ³é¢‘æ–‡ä»¶è¿›è¡Œæ¨ç†

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
            warmup_runs: é¢„çƒ­æ¬¡æ•°
            benchmark_runs: åŸºå‡†æµ‹è¯•æ¬¡æ•°

        Returns:
            æ¨ç†ç»“æœå­—å…¸ï¼ˆåŒ…å«ç³»ç»Ÿé…ç½®ä¿¡æ¯ï¼‰
        """
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
        if self.encoder_session is None or self.decoder_session is None:
            return {"error": "ç¼–ç å™¨æˆ–è§£ç å™¨æœªåŠ è½½"}

        try:
            # 1. åŠ è½½å¹¶å¤„ç†éŸ³é¢‘
            speech_np, speech_lengths_np, audio_duration = self._load_and_process_audio(audio_path)

            # 2. é¢„çƒ­è¿è¡Œï¼ˆé¿å…é¦–æ¬¡æ¨ç†è€—æ—¶åé«˜ï¼‰
            if warmup_runs > 0:
                logging.info(f"å¼€å§‹é¢„çƒ­è¿è¡Œï¼ˆ{warmup_runs}æ¬¡ï¼‰...")
                for i in range(warmup_runs):
                    try:
                        # ç¼–ç å™¨æ¨ç†
                        encoder_inputs = {"speech": speech_np, "speech_lengths": speech_lengths_np}
                        encoder_outputs = self.encoder_session.run(None, encoder_inputs)
                        encoder_out_np = encoder_outputs[0]
                        encoder_out_lens_np=encoder_outputs[1]

                        # è°ƒæ•´é•¿åº¦
                        if self.target_seq_len>0:
                            encoder_out_np, encoder_out_lens_np = self._pad_or_truncate_encoder_output(encoder_out_np)

                        # è§£ç å™¨æ¨ç†
                        decoder_inputs = {"encoder_out": encoder_out_np, "encoder_out_lens": encoder_out_lens_np}
                        self.decoder_session.run(None, decoder_inputs)
                    except Exception as e:
                        logging.warning(f"é¢„çƒ­è¿è¡Œå‡ºé”™: {e}")
                logging.info("é¢„çƒ­å®Œæˆ")

            # 3. åŸºå‡†æµ‹è¯•
            infer_times = []
            encoder_times = []
            decoder_times = []
            ctc_logits = None
            output_lengths = None

            logging.info(f"å¼€å§‹åŸºå‡†æµ‹è¯•ï¼ˆ{benchmark_runs}æ¬¡ï¼‰...")
            for i in range(benchmark_runs):
                start_time = time.time()

                # ç¼–ç å™¨æ¨ç†
                enc_start = time.time()
                encoder_inputs = {"speech": speech_np, "speech_lengths": speech_lengths_np}
                encoder_outputs = self.encoder_session.run(None, encoder_inputs)
                encoder_out_np = encoder_outputs[0]
                encoder_out_lens_np = encoder_outputs[1]
                enc_time = time.time() - enc_start
                encoder_times.append(enc_time)

                # è°ƒæ•´ç¼–ç å™¨è¾“å‡ºé•¿åº¦
                if self.target_seq_len>0:
                    encoder_out_np, encoder_out_lens_np = self._pad_or_truncate_encoder_output(encoder_out_np)

                # è§£ç å™¨æ¨ç†
                dec_start = time.time()
                decoder_inputs = {"encoder_out": encoder_out_np, "encoder_out_lens": encoder_out_lens_np}
                decoder_outputs = self.decoder_session.run(None, decoder_inputs)
                ctc_logits = decoder_outputs[0]
                output_lengths = decoder_outputs[1]
                dec_time = time.time() - dec_start
                decoder_times.append(dec_time)

                # æ€»æ¨ç†æ—¶é—´
                total_time = time.time() - start_time
                infer_times.append(total_time)

            # è®¡ç®—å¹³å‡è€—æ—¶
            avg_infer_time = np.mean(infer_times)
            avg_encoder_time = np.mean(encoder_times)
            avg_decoder_time = np.mean(decoder_times)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.inference_stats.update({
                "encoder_infer_time": avg_encoder_time,
                "decoder_infer_time": avg_decoder_time,
                "total_infer_time": avg_infer_time
            })

            logging.info(f"åŸºå‡†æµ‹è¯•å®Œæˆ:")
            logging.info(f"  å¹³å‡æ¨ç†è€—æ—¶: {avg_infer_time:.3f}s (Â±{np.std(infer_times):.3f})")
            logging.info(f"  å¹³å‡ç¼–ç å™¨è€—æ—¶: {avg_encoder_time:.3f}s")
            logging.info(f"  å¹³å‡è§£ç å™¨è€—æ—¶: {avg_decoder_time:.3f}s")

            # 4. è§£ç ä¸ºæ–‡æœ¬
            decode_start = time.time()
            results = self._decode_ctc_logits(ctc_logits, output_lengths, tokenizer)
            self.inference_stats["decode_time"] = time.time() - decode_start

            # 5. è®¡ç®—RTF
            rtf = self.calculate_rtf(avg_infer_time, audio_duration)

            # 6. æ•´ç†å¹¶è¿”å›ç»“æœï¼ˆåŒ…å«ç³»ç»Ÿé…ç½®ä¿¡æ¯ï¼‰
            return {
                # åŸºç¡€ä¿¡æ¯
                "audio_path": audio_path,
                "audio_duration": audio_duration,
                "audio_sample_rate": self.audio_sample_rate,
                "speech_features_shape": speech_np.shape,
                "encoder_output_shape": encoder_out_np.shape,
                "ctc_logits_shape": ctc_logits.shape if ctc_logits is not None else None,

                # æ¨ç†ç»“æœ
                "predictions": results,

                # æ€§èƒ½æŒ‡æ ‡
                "inference_time": avg_infer_time,
                "encoder_time": avg_encoder_time,
                "decoder_time": avg_decoder_time,
                "feature_extract_time": self.inference_stats["feature_extract_time"],
                "audio_load_time": self.inference_stats["audio_load_time"],
                "decode_time": self.inference_stats["decode_time"],
                "rtf": rtf,
                "batch_size": speech_np.shape[0],
                "benchmark_runs": benchmark_runs,
                "infer_time_std": np.std(infer_times),

                # ç³»ç»Ÿé…ç½®
                "device_type": self.device_type,
                "intra_op_num_threads": self.intra_op_num_threads,
                "inter_op_num_threads": self.inter_op_num_threads
            }

        except Exception as e:
            logging.error(f"éŸ³é¢‘æ¨ç†å¤±è´¥: {e}")
            logging.error(traceback.format_exc())
            return {"error": str(e), "audio_path": audio_path}


# ===================== å·¥å…·å‡½æ•° =====================
def get_tokenizer(tokenizer_dir: str) -> Optional[object]:
    """è·å–tokenizerï¼ˆç”¨äºæ¨ç†ï¼‰"""
    try:
        vocab_path = os.path.join(tokenizer_dir, "multilingual.tiktoken")
        tokenizer = SenseVoiceTokenizer(
            language="en",
            task="transcribe",
            vocab_path=vocab_path
        )
        return tokenizer
    except Exception as e:
        logging.warning(f"è·å–tokenizerå¤±è´¥: {e}")
        return None


# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½ONNXæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†æµ‹è¯•"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # 1. æ£€æŸ¥ONNXæ¨¡å‹è·¯å¾„
    encoder_onnx_path = os.path.join(CONFIG["onnx_model_dir"], "encoder.onnx")
    decoder_onnx_path = os.path.join(CONFIG["onnx_model_dir"], "decoder.onnx")
    encoder_int8_path = os.path.join(CONFIG["onnx_model_dir"], "encoder.int8.onnx")
    decoder_int8_path = os.path.join(CONFIG["onnx_model_dir"], "decoder.int8.onnx")

    # æ£€æŸ¥å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(encoder_onnx_path):
        logging.error(f"ç¼–ç å™¨ONNXæ¨¡å‹ä¸å­˜åœ¨: {encoder_onnx_path}")
        # return

    if not os.path.exists(decoder_onnx_path):
        logging.error(f"è§£ç å™¨ONNXæ¨¡å‹ä¸å­˜åœ¨: {decoder_onnx_path}")
        # return

    # 2. è·å–tokenizer
    tokenizer = get_tokenizer(CONFIG["tokenizer_dir"])

    # 3. æ£€æŸ¥æµ‹è¯•éŸ³é¢‘
    if not os.path.exists(CONFIG["audio_test_path"]):
        logging.error(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['audio_test_path']}")
        print(f"\nâŒ é”™è¯¯ï¼šéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {CONFIG['audio_test_path']}")
        return

    # 4. FP32 ONNXæ¨¡å‹æ¨ç†æµ‹è¯•
    print("\n" + "=" * 50)
    print("FP32 ONNXæ¨¡å‹æ¨ç†æµ‹è¯•")
    print("=" * 50)

    inference_original = CTCInference(
        encoder_onnx_path=encoder_onnx_path,
        decoder_onnx_path=decoder_onnx_path,
        blank_id=CONFIG["blank_id_default"],
        target_seq_len=CONFIG["target_seq_len"]
    )

    results_original = inference_original.inference_from_audio(
        audio_path=CONFIG["audio_test_path"],
        tokenizer=tokenizer,
        warmup_runs=CONFIG["warmup_runs"],
        benchmark_runs=CONFIG["benchmark_runs"]
    )

    # æ‰“å°FP32 ONNXæ¨¡å‹ç»“æœï¼ˆåŒ…å«æ–°å¢çš„ç³»ç»Ÿä¿¡æ¯ï¼‰
    if "error" not in results_original:
        print(f"\nğŸ“Š FP32 ONNXæ¨¡å‹:")
        print(f"  ğŸ–¥ï¸  è®¾å¤‡ç±»å‹: {results_original['device_type']}")
        print(
            f"  ğŸ§µ  çº¿ç¨‹é…ç½®: intra_op={results_original['intra_op_num_threads']}, inter_op={results_original['inter_op_num_threads']}")
        print(
            f"  ğŸµ  éŸ³é¢‘ä¿¡æ¯: æ—¶é•¿={results_original['audio_duration']:.2f}s, é‡‡æ ·ç‡={results_original['audio_sample_rate']}Hz")
        print(f"  â±ï¸  æ¨ç†è€—æ—¶: {results_original['inference_time']:.3f}s (Â±{results_original['infer_time_std']:.3f})")
        print(f"  ğŸš€  RTF: {results_original['rtf']:.4f}")
        print(f"  ğŸ“  è§£ç ç»“æœ: {results_original['predictions'][0]['text']}")
    else:
        print(f"\nâŒ FP32 ONNXæ¨¡å‹æ¨ç†å¤±è´¥: {results_original['error']}")

    # 5. é‡åŒ–æ¨¡å‹æ¨ç†æµ‹è¯•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(encoder_int8_path) and os.path.exists(decoder_int8_path):
        print("\n" + "=" * 50)
        print("INT8é‡åŒ–ONNXæ¨¡å‹æ¨ç†æµ‹è¯•ï¼ˆç¼–ç å™¨+è§£ç å™¨ï¼‰")
        print("=" * 50)

        inference_int8 = CTCInference(
            encoder_onnx_path=encoder_int8_path,  # ä½¿ç”¨é‡åŒ–çš„ç¼–ç å™¨
            decoder_onnx_path=decoder_int8_path,
            blank_id=CONFIG["blank_id_default"],
            target_seq_len=CONFIG["target_seq_len"]
        )

        results_int8 = inference_int8.inference_from_audio(
            audio_path=CONFIG["audio_test_path"],
            tokenizer=tokenizer,
            warmup_runs=CONFIG["warmup_runs"],
            benchmark_runs=CONFIG["benchmark_runs"]
        )

        # æ‰“å°é‡åŒ–æ¨¡å‹ç»“æœï¼ˆåŒ…å«æ–°å¢çš„ç³»ç»Ÿä¿¡æ¯ï¼‰
        if "error" not in results_int8:
            print(f"\nğŸ“Š INT8é‡åŒ–æ¨¡å‹:")
            print(f"  ğŸ–¥ï¸  è®¾å¤‡ç±»å‹: {results_int8['device_type']}")
            print(
                f"  ğŸ§µ  çº¿ç¨‹é…ç½®: intra_op={results_int8['intra_op_num_threads']}, inter_op={results_int8['inter_op_num_threads']}")
            print(
                f"  ğŸµ  éŸ³é¢‘ä¿¡æ¯: æ—¶é•¿={results_int8['audio_duration']:.2f}s, é‡‡æ ·ç‡={results_int8['audio_sample_rate']}Hz")
            print(f"  â±ï¸  æ¨ç†è€—æ—¶: {results_int8['inference_time']:.3f}s (Â±{results_int8['infer_time_std']:.3f})")
            print(f"  ğŸš€  RTF: {results_int8['rtf']:.4f}")
            print(f"  ğŸ“  è§£ç ç»“æœ: {results_int8['predictions'][0]['text']}")

            # æ€§èƒ½å¯¹æ¯”
            if "error" not in results_original:
                speedup = (results_original['inference_time'] - results_int8['inference_time']) / results_original[
                    'inference_time'] * 100
                rtf_improvement = (results_original['rtf'] - results_int8['rtf']) / results_original['rtf'] * 100

                print("\n" + "=" * 50)
                print("æ€§èƒ½å¯¹æ¯”ç»“æœ")
                print("=" * 50)
                print(f"\nğŸš€ æ€§èƒ½å¯¹æ¯”:")
                print(f"  ğŸš„  æ¨ç†é€Ÿåº¦æå‡: {speedup:.1f}%")
                print(f"  ğŸ“‰  RTFæ”¹å–„: {rtf_improvement:.1f}%")
        else:
            print(f"\nâŒ é‡åŒ–æ¨¡å‹æ¨ç†å¤±è´¥: {results_int8['error']}")
    elif os.path.exists(decoder_int8_path):
        # å…¼å®¹ä»…è§£ç å™¨é‡åŒ–çš„æƒ…å†µ
        print("\n" + "=" * 50)
        print("INT8é‡åŒ–ONNXæ¨¡å‹æ¨ç†æµ‹è¯•ï¼ˆä»…è§£ç å™¨ï¼‰")
        print("=" * 50)

        inference_int8 = CTCInference(
            encoder_onnx_path=encoder_onnx_path,
            decoder_onnx_path=decoder_int8_path,
            blank_id=CONFIG["blank_id_default"],
            target_seq_len=CONFIG["target_seq_len"]
        )

        results_int8 = inference_int8.inference_from_audio(
            audio_path=CONFIG["audio_test_path"],
            tokenizer=tokenizer,
            warmup_runs=CONFIG["warmup_runs"],
            benchmark_runs=CONFIG["benchmark_runs"]
        )

        # æ‰“å°é‡åŒ–æ¨¡å‹ç»“æœ
        if "error" not in results_int8:
            print(f"\nğŸ“Š INT8é‡åŒ–æ¨¡å‹ï¼ˆä»…è§£ç å™¨ï¼‰:")
            print(f"  ğŸ–¥ï¸  è®¾å¤‡ç±»å‹: {results_int8['device_type']}")
            print(
                f"  ğŸ§µ  çº¿ç¨‹é…ç½®: intra_op={results_int8['intra_op_num_threads']}, inter_op={results_int8['inter_op_num_threads']}")
            print(
                f"  ğŸµ  éŸ³é¢‘ä¿¡æ¯: æ—¶é•¿={results_int8['audio_duration']:.2f}s, é‡‡æ ·ç‡={results_int8['audio_sample_rate']}Hz")
            print(f"  â±ï¸  æ¨ç†è€—æ—¶: {results_int8['inference_time']:.3f}s (Â±{results_int8['infer_time_std']:.3f})")
            print(f"  ğŸš€  RTF: {results_int8['rtf']:.4f}")
            print(f"  ğŸ“  è§£ç ç»“æœ: {results_int8['predictions'][0]['text']}")

            # æ€§èƒ½å¯¹æ¯”
            if "error" not in results_original:
                speedup = (results_original['inference_time'] - results_int8['inference_time']) / results_original[
                    'inference_time'] * 100
                rtf_improvement = (results_original['rtf'] - results_int8['rtf']) / results_original['rtf'] * 100

                print("\n" + "=" * 50)
                print("æ€§èƒ½å¯¹æ¯”ç»“æœ")
                print("=" * 50)
                print(f"\nğŸš€ æ€§èƒ½å¯¹æ¯”:")
                print(f"  ğŸš„  æ¨ç†é€Ÿåº¦æå‡: {speedup:.1f}%")
                print(f"  ğŸ“‰  RTFæ”¹å–„: {rtf_improvement:.1f}%")
        else:
            print(f"\nâŒ é‡åŒ–æ¨¡å‹æ¨ç†å¤±è´¥: {results_int8['error']}")
    else:
        logging.info("INT8é‡åŒ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡é‡åŒ–æ¨¡å‹æµ‹è¯•")
        if not os.path.exists(encoder_int8_path):
            logging.info(f"ç¼–ç å™¨INT8æ¨¡å‹ä¸å­˜åœ¨: {encoder_int8_path}")
        if not os.path.exists(decoder_int8_path):
            logging.info(f"è§£ç å™¨INT8æ¨¡å‹ä¸å­˜åœ¨: {decoder_int8_path}")


if __name__ == "__main__":
    main()